---
title: Reddit x 100
subtitle: Key Plots
author: Molly Lewis 
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    number_sections: false
    theme: cerulean
    toc_float: true
    code_folding: hide
---
  
  
```{r setup, include = F}
# load packages
library(tidyverse) 
library(knitr)
library(here)
library(anytime)
library(broom)
library(data.table)
library(viridis)
library(ggrepel)


opts_chunk$set(echo = T, message = F, warning = F, 
               error = F, tidy = F,  cache = F, fig.height = 5, fig.width = 5)

get_power_law_exponent <- function(df, predictor, outcome){
  model <- lm(unlist(log10(df[,outcome])) ~ unlist(log10(df[,predictor])))
  
  all_parameters<- model %>%
    tidy() %>%
    mutate(term = c("intercept", predictor))
  
  conf <- confint(model) %>%
    data.frame() %>%
    slice(2) %>%
    mutate_all(round, 2)
  
  power_law_exp <- all_parameters[2, "estimate"] %>% 
    unlist(use.names = F) %>%
    round(2)
  
  power_law_exp_print <- paste0(power_law_exp, "\n[95% CI: ",  conf[1], ", ",  conf[2], "]")
  
  list(power_law_exp, all_parameters, power_law_exp_print)
}
``` 


```{r}

TYPE_PATH <- here("data/word_type_counts.csv")
type_counts <- read_csv(TYPE_PATH)

AUTHOR_COUNTS <-  here("exploratory_analyses/03_systematic_sample/data/subreddit_counts_scores.csv")
author_count_measures <- read_csv(AUTHOR_COUNTS, col_names = c("subreddit","author_n","word_H","word_mean_n","word_sd","word_total","score_mean",                                                    "score_sd","score_H","comments_n_long","comments_n_all",
                                                               "posts_n_all","comments_posts_ratio")) %>%
  filter(author_n > 100) %>%
  arrange(-author_n) %>%
  mutate(author_rank = 1:n()) %>%
    filter(!(subreddit == "newsokur")) %>%
  left_join(type_counts)


author_counts <- author_count_measures %>%
  select(subreddit, author_n, author_rank)
```



```{r}
#Implications for language change:
#* Larger communnities: more homogenous input because of inequality across speakers
# More consensus -> faster word gain.
```


## Language counts{.tabset}

More people, more words.
(there's no relationship between mean/sd post length and author n.)

###  Types

```{r}


exp_value <- get_power_law_exponent(author_count_measures, "author_n","word_total")[[1]]


label_data <- author_count_measures %>%
  filter(subreddit %in% c("DisneyTravel", "movies", "neoliberal", "JustCause", "FinancialCareers"))

#pdf(here("/exploratory_analyses/03_systematic_sample/plots/nwords.pdf"),width = 6, height = 6)

ggplot(author_count_measures, aes(x = author_n, y = word_total)) +
  geom_point(size = 4, alpha = .4) +
  geom_smooth(method = "lm") +
    geom_abline(slope = 1, linetype = 2) +

  ggtitle("Number of Words vs. Community Size") +
  geom_text_repel(data = label_data, aes(label = subreddit), min.segment.length = .2) +
  scale_y_log10(name = "N total words (log)",
                labels = scales::trans_format("log10", scales::math_format(10^.x))) +
  scale_x_log10(name = "N authors (log)",
                labels = scales::trans_format("log10", scales::math_format(10^.x)))+
  annotation_logticks() +
  annotate("text", label = paste0("Exp =\n", exp_value ),
           x = 100000, y = 100000, color= "red", size = 5) +
  theme_classic(base_size = 16)
#dev.off()


```

## Word type count
```{r}

exp_value2 <- get_power_law_exponent(author_count_measures, "author_n","n_word_types")[[3]]

#pdf(here("/exploratory_analyses/03_systematic_sample/plots/nword_types.pdf"), 
#    width = 6, height = 6)

ggplot(author_count_measures, aes(x = author_n, y = n_word_types)) +
  geom_point(size = 4, alpha = .4) +
  geom_smooth(method = "lm")+
  ggtitle("Number of Word Types vs. Community Size") +
  geom_text_repel(data = label_data, aes(label = subreddit), min.segment.length = .2) +
  scale_y_log10(name = "N word types (log)",
                labels = scales::trans_format("log10", scales::math_format(10^.x)),
                breaks = c(10000, 100000, 1000000)) +
  geom_abline(slope = 1, linetype = 2, intercept = 2.58) +
  scale_x_log10(name = "N authors (log)",
                labels = scales::trans_format("log10", scales::math_format(10^.x)))+
  annotation_logticks() +
  
  annotate("text", label = paste0("Exp = ", exp_value2), x = 100000, y = 10000, color= "red", size = 5) +
  theme_classic(base_size = 16)
#dev.off()

```

## Token to type ratio
```{r}

author_count_measures_with_token_type <- author_count_measures %>%
  mutate(token_type_ratio = word_total/n_word_types)

exp_value3 <- get_power_law_exponent(author_count_measures_with_token_type, "author_n","token_type_ratio")[[3]]

ggplot(author_count_measures_with_token_type, aes(x = author_n, y = token_type_ratio))  +
  geom_point(size = 4, alpha = .4) +
  geom_smooth(method = "lm")+
  ggtitle("Number of Word Types vs. Community Size") +
  #geom_text_repel(data = label_data, aes(label = subreddit), min.segment.length = .2) +
  scale_y_log10(name = "Token:Type (log)",
                labels = scales::trans_format("log10", scales::math_format(10^.x))) +
  geom_abline(slope = 1, linetype = 2, intercept = 2.58) +
  scale_x_log10(name = "N authors (log)",
                labels = scales::trans_format("log10", scales::math_format(10^.x)))+
  annotate("text", label = paste0("Exp = ", exp_value3), x = 100000, y = 10000, color= "red", size = 5) +
  geom_abline(slope = 1, linetype = 2, intercept = -0.0833) +
  annotation_logticks() +
    theme_classic(base_size = 16)

```


## Author longevity

```{r}
AUTHOR_TIME <- here("exploratory_analyses/03_systematic_sample/data/subreddit_author_time_data.csv")
author_time <- read_csv(AUTHOR_TIME, col_names = c("subreddit", "author_longevity_mean", "author_sd_mean",
                                                   "author_longevity_H", "author_lag_sd", "author_lag_H", "author_lag_mean")) %>%
    left_join(author_counts) %>%
     filter(author_n > 100)  


```


Time from first to last post - bigger communities have people with longer lifespans. 
```{r}
scientific_10 <- function(x) {
  parse(text=gsub("e", "%*%10^", scales::scientific_format()(x)))
}

label_data2 <- author_time %>%
  filter(subreddit %in% c("DisneyTravel", "movies", "neoliberal", "JustCause", "FinancialCareers"))

longevity_corr <- cor.test(log(author_time$author_n), author_time$author_longevity_mean) %>%
  pluck("estimate") %>%
  pluck("cor") %>%
  round(2)

pdf(here("/exploratory_analyses/03_systematic_sample/plots/longevity.pdf"),width = 6, height = 6)
ggplot(author_time, aes(x = author_n, y = author_longevity_mean)) +
  geom_point(size = 4, alpha = .4) +
  geom_smooth(method = "lm")+
  ggtitle("Author Lifespan vs. Community Size") +
  geom_text_repel(data = label_data2, aes(label = subreddit), , min.segment.length = .2) +
  scale_y_continuous(name = "Mean author lifespan (sec.)", label = scientific_10) +
  annotate("text", label = paste0("r = ", longevity_corr ),
           x = 100000, y = 5000000, color= "red", size = 6) +
  scale_x_log10(name = "N authors (log)",
                labels = scales::trans_format("log10", scales::math_format(10^.x)))+
  annotation_logticks(sides = "b") +
  theme_classic(base_size = 16)
dev.off()
```


## Churn
For each subreddit, for each week, calculated in/(in + out). Where "in" = first time posting to community; "out" = last time posting in community. The data below is for a measure where I only consider all comments (not restricitng on length). Each panel shows a subreddit with the red dashed line indicating the mean in-churn over time. Red lines greater than .5 indicate that a community is growing. Note that we're somewhat underestimating overall growth here by including the last time period (where everyone dies). But, this is the same across communities.

```{r}
CHURN_PATH <- here("exploratory_analyses/03_systematic_sample/data/churn_overtime.csv")

churn <- read_csv(CHURN_PATH, col_names = c("subreddit", "created_bin", "in_churn",
                                            "inout_sum", "comment_length_type")) %>%
  filter(comment_length_type == "all") %>%
  left_join(author_counts) %>%
  mutate(subreddit = fct_reorder(subreddit, author_n))  %>%
  filter(author_n > 100) 

in_churn <- churn %>%
  group_by(subreddit) %>%
  summarize(mean_churn = mean(in_churn))
```


## Churn over time
```{r,  fig.height = 6, fig.width = 7}
pdf(here("/exploratory_analyses/03_systematic_sample/plots/churn1.pdf"),width = 7.5, height = 6)

churn %>%
  ggplot(aes(x = created_bin, y = in_churn, color = log10(author_n), 
              group = subreddit)) +
  scale_color_viridis(alpha = .5, direction = -1) +
  geom_smooth(se = F) +
  ggtitle("Author Churn vs. Community Size") +
  xlab("Week") +
  ylab("Churn-in") +
  labs(color = "N Authors (log)") +
  theme_classic(base_size = 16)
dev.off()

```

## Mean churn
```{r}
churn_with_author <- in_churn %>%
  left_join(author_counts) 

label_data3 <- churn_with_author  %>%
  filter(subreddit %in% c("DisneyTravel", "movies", "neoliberal", "JustCause", "FinancialCareers"))


churn_corr <- cor.test(log(churn_with_author$author_n), churn_with_author$mean_churn) %>%
  pluck("estimate") %>%
  pluck("cor") %>%
  round(2)

pdf(here("/exploratory_analyses/03_systematic_sample/plots/churn2.pdf"),width = 6, height = 6)

ggplot(churn_with_author, aes(x = author_n, y = mean_churn)) +
  geom_point(size = 4, alpha = .4) +
  geom_text_repel(data = label_data3, aes(label = subreddit)) +
  ggtitle("Mean Author Churn vs. Community Size") +
  ylab("Churn-in") +
  annotate("text", label = paste0("r = ", churn_corr ),
           x = 1000, y = .45, color= "red", size = 6) +
  geom_smooth(method = "lm") +
  ylab("Mean churn-in") +
  scale_x_log10(name = "N authors (log)",
                labels = scales::trans_format("log10", scales::math_format(10^.x)))+
  annotation_logticks(sides = "b") +
  theme_classic(base_size = 16)
dev.off()

```



##  Author inequality


Long comments only
```{r}

AUTHOR_INEQ_LONG <- here("exploratory_analyses/03_systematic_sample/data/subreddit_author_inequality_long.csv")

author_ineq_long <- read_csv(AUTHOR_INEQ_LONG, col_names = c("subreddit", "comment_author_H",
                                                             "comment_gini_coeff",
                                     "comment_normalized_author_H")) %>%
  left_join(author_counts) %>%
  mutate(subreddit = fct_reorder(subreddit, author_n)) %>%
  filter(author_n > 100) 


label_data4 <- author_ineq_long  %>%
  filter(subreddit %in% c("DisneyTravel", "movies", "neoliberal", "JustCause", "FinancialCareers"))


gini_corr <- cor.test(log(author_ineq_long$author_n), author_ineq_long$comment_gini_coeff) %>%
  pluck("estimate") %>%
  pluck("cor") %>%
  round(2)

pdf(here("/exploratory_analyses/03_systematic_sample/plots/gini.pdf"),width = 6, height = 6)

ggplot(author_ineq_long, aes(x = author_n, y = comment_gini_coeff)) +
  geom_point(size = 4, alpha = .4) +
  geom_text_repel(data = label_data4, aes(label = subreddit)) +
  ggtitle("Distribution of Comments over Authors vs.\nCommunity Size") +
  geom_smooth(method = "lm")+
  ylab("Gini Coefficient") +
    annotate("text", label = paste0("r = ", gini_corr ),
           x = 100000, y = .35, color= "red", size = 6) +
  scale_x_log10(name = "N authors (log)",
                labels = scales::trans_format("log10", scales::math_format(10^.x)))+
  theme_classic() +
  annotation_logticks(sides = "b") +
  theme_classic(base_size = 16)

dev.off()
```


```{r, include = F}
AUTHOR_COUNTS <-  here("exploratory_analyses/03_systematic_sample/data/subreddit_counts_scores.csv")
author_count_measures <- read_csv(AUTHOR_COUNTS, col_names = c("subreddit","author_n","word_H","word_mean_n","word_sd","word_total","score_mean",                                                    "score_sd","score_H","comments_n_long","comments_n_all",
                                                               "posts_n_all","comments_posts_ratio"))

AUTHOR_TIME <- here("exploratory_analyses/03_systematic_sample/data/subreddit_author_time_data.csv")
author_time <- read_csv(AUTHOR_TIME, col_names = c("subreddit", "author_longevity_mean", "author_sd_mean",
                                                   "author_longevity_H", "author_lag_sd", "author_lag_H", "author_lag_mean")) 
```

```{r}
# language variables
LCU_PATH <- "/Volumes/wilbur_the_great/LANGSCALES_subreddit_sample/lau_by_author_comment/"
lcu_measures <- map_df(list.files(LCU_PATH, full.names = T), read_csv) 

lcu_measures_tidy <- lcu_measures %>%
  select(subreddit, everything()) %>%
  rename(lcu = lau) %>%
  filter(!(subreddit == "newsokur"))

lau_measures <- lcu_measures_tidy %>%
  group_by(subreddit, author) %>%
  summarize(lau = mean(lcu))
```

```{r}
# demo variables
comment_counts <- lcu_measures_tidy %>%
  count(subreddit, name = "comment_n")

word_counts <- lcu_measures_tidy %>%
  group_by(subreddit) %>%
  summarize(mean_word_n = mean(n_post_words),
            sum_word_n = sum(n_post_words))

actual_author_counts <- lcu_measures_tidy %>%
  distinct(subreddit, author) %>%
  count(subreddit, name = "actual_author_n")
```

# LSU
```{r}
all_subreddit_measures <- lau_measures %>%
  group_by(subreddit)  %>%
  summarize(lsu = mean(lau)) %>%
  left_join(comment_counts) %>%
  left_join(word_counts) %>%
  left_join(actual_author_counts) %>%
  filter(actual_author_n >= 10) %>%
  left_join(author_time) 
```


```{r}
label_data5 <- all_subreddit_measures  %>%
  filter(subreddit %in% c("DisneyTravel", "movies", "neoliberal", "JustCause", "FinancialCareers"))


pdf(here("/exploratory_analyses/03_systematic_sample/plots/lsu.pdf"),width = 6, height = 6)

ggplot(all_subreddit_measures, aes(x = actual_author_n, y = -lsu)) +
  geom_point(size = 4, alpha = .4) +
  ylab("Log Subreddit Uniqueness") +
  ggtitle("Language Uniqueness vs. Community Size") +
  geom_text_repel(data = label_data5, aes(label = subreddit)) +
  geom_smooth(method= "lm") +
  xlab("Log number of authors") +
  scale_x_log10(name = "N authors (log)",
                labels = scales::trans_format("log10", scales::math_format(10^.x)))+
  annotation_logticks(sides = "b") +
  theme_classic(base_size = 16) 
dev.off()


```

## LCU


# LCU
LCU as a function of nth post by a users.
```{r, fig.height = 8, fig.width = 8}
mean_lcu <- lcu_measures_tidy %>%
  nest(-nth_comment, -subreddit) %>%
  mutate(num_comments = map_dbl(data, nrow)) %>%
  filter(num_comments > 5) %>% # can't calculate
  mutate(test = map(data, ~t.test(.x$lcu)),
         tidied  = map(test, tidy)) %>%
  unnest(tidied, drop = T) %>%
  select(subreddit, nth_comment, num_comments,
         estimate, conf.low, conf.high)  %>%
  filter(nth_comment <= 50) %>%
  left_join(actual_author_counts) %>%
  mutate(subreddit = fct_reorder(subreddit, actual_author_n))

ggplot(mean_lcu, aes(x = nth_comment, y = estimate)) +
 # geom_point() +
  facet_wrap(~subreddit) +
  geom_smooth()+
  theme_classic()
```

```{r}
mean_lcu %>%
  filter(actual_author_n > 500) %>%
  ggplot(aes(x = nth_comment, y = -estimate, 
           group = subreddit, color = log(actual_author_n))) +
  scale_color_viridis(alpha = .8) +
  #geom_point() +
  #facet_wrap(~subreddit) +
  geom_smooth(se = F, size = 3)+
  theme_classic()

mean_lcu_slope <- mean_lcu %>% # can't calculate
  nest(-subreddit) %>%
  mutate(test = map(data, ~lm(estimate ~ nth_comment, data = .x)),
         tidied  = map(test, tidy)) %>%
  select(-test, -data) %>%
  unnest() %>%
  filter(term == "nth_comment") %>%
  left_join(mean_lcu %>% distinct(subreddit, actual_author_n))

mean_lcu_slope %>%
  filter(estimate > -.07) %>%
  filter(estimate < .03) %>%

  ggplot(aes(y = estimate , x = log(actual_author_n))) +
  geom_label(aes(label = subreddit)) +
  scale_color_viridis(alpha = .8) +
  geom_point() +
  geom_smooth(method = "lm")+
  theme_classic()


```


